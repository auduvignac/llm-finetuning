{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "g3PmJq6pQqbP",
   "metadata": {
    "id": "g3PmJq6pQqbP"
   },
   "source": [
    "# *Fine-tuning* de *Large Language Models* (LLMs)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"https://colab.research.google.com/github/auduvignac/llm-finetuning/blob/main/notebooks/project/finetuning-projet.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Ouvrir dans Google Colab\"/>\n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "Le but de ce projet est de r√©aliser le *fine-tuning* d'un LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9-t5uVFuNQiF",
   "metadata": {
    "id": "9-t5uVFuNQiF"
   },
   "source": [
    "## Installation des biblioth√®ques/libraires requises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "-wuPJgZriD29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wuPJgZriD29",
    "outputId": "a112907a-a6a7-4a47-fe20-49206b4395e4",
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Ex√©cution sur Colab : v√©rification stricte des d√©pendances‚Ä¶\n",
      "üîÅ accelerate 1.10.0 ne satisfait pas '==1.8.1' ‚Üí r√©installation : accelerate==1.8.1\n",
      "‚¨áÔ∏è Manquant : bitsandbytes==0.46.0\n",
      "‚úÖ datasets 4.0.0 ‚Äî OK\n",
      "‚úÖ matplotlib 3.10.0 ‚Äî OK\n",
      "‚úÖ numpy 2.0.2 ‚Äî OK\n",
      "üîÅ peft 0.17.0 ne satisfait pas '==0.15.0' ‚Üí r√©installation : peft==0.15.0\n",
      "‚úÖ tabulate 0.9.0 ‚Äî OK\n",
      "‚úÖ torch 2.8.0+cu126 ‚Äî OK\n",
      "‚úÖ tqdm 4.67.1 ‚Äî OK\n",
      "‚úÖ transformers 4.55.2 ‚Äî OK\n",
      "‚¨áÔ∏è Installation/r√©ininstallation des paquets n√©cessaires‚Ä¶\n",
      " $ /usr/bin/python3 -m pip install accelerate==1.8.1\n",
      " $ /usr/bin/python3 -m pip install bitsandbytes==0.46.0\n",
      " $ /usr/bin/python3 -m pip install peft==0.15.0\n"
     ]
    }
   ],
   "source": [
    "!wget -q https://raw.githubusercontent.com/auduvignac/llm-finetuning/refs/heads/main/setup_env.py -O setup_env.py\n",
    "%run setup_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nCALzgA2Nleg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nCALzgA2Nleg",
    "outputId": "c05a91c8-a179-41a1-fc25-376b85ed3c80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from tabulate import (\n",
    "    tabulate,\n",
    ")\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    ")\n",
    "from tqdm import (\n",
    "    tqdm,\n",
    ")\n",
    "\n",
    "# Si le notebook est ex√©cut√© dans un environnement jupyter, la librairie\n",
    "# ci-dessus peut √™tre utilis√©e\n",
    "from tqdm.notebook import (\n",
    "    tqdm,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DistilBertConfig,\n",
    "    DistilBertModel,\n",
    "    DistilBertTokenizer,\n",
    ")\n",
    "\n",
    "# Utilisation d‚Äôun GPU avec CUDA lorsque disponible sur la machine d‚Äôex√©cution.\n",
    "# L‚Äôutilisation d‚Äôun GPU pour l‚Äôapprentissage entra√Æne souvent d‚Äô√©normes\n",
    "# acc√©l√©rations lors de l‚Äôentra√Ænement.\n",
    "# Voir https://developer.nvidia.com/cuda-downloads pour installer CUDA\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YX7e3F_BR0BU",
   "metadata": {
    "id": "YX7e3F_BR0BU"
   },
   "outputs": [],
   "source": [
    "class LlamaFineTuner:\n",
    "  def __init__(self, model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"):\n",
    "    self.model_id = model_id\n",
    "    self.bnb_config = BitsAndBytesConfig(\n",
    "      load_in_4bit=True,\n",
    "      bnb_4bit_quant_type=\"nf4\",\n",
    "      bnb_4bit_compute_dtype=\"float16\",\n",
    "      bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "    self.model = AutoModelForCausalLM.from_pretrained(\n",
    "        self.model_id,\n",
    "        quantization_config=self.bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lcK0ijcxQncc",
   "metadata": {
    "id": "lcK0ijcxQncc"
   },
   "source": [
    "## Classification de textes avec les Transformers\n",
    "\n",
    "Ce projet porte sur une t√¢che de **classification de textes** appliqu√©e au **jeu de donn√©es IMDB** (analyse de sentiments ou *sentimental analysis*).  \n",
    "Ce travail s'appuiera sur les **architectures de type encodeur**, en particulier l‚Äôun des mod√®les les plus connus : **BERT** (et sa variante l√©g√®re **DistilBERT**).\n",
    "\n",
    "## Pr√©sentation\n",
    "\n",
    "Le projet consiste √† utiliser la librairie `datasets` pour le chargement des donn√©es et les `tokenizers` pour le pr√©traitement des textes.\n",
    "\n",
    "La librairie **Transformers** propose une API simple pour utiliser des mod√®les pr√©-entra√Æn√©s tels que **BERT** ou **GPT**. Elle facilite leur t√©l√©chargement, leur r√©-entra√Ænement et leur int√©gration, tout en r√©duisant les co√ªts de calcul et en restant compatible avec **PyTorch, TensorFlow et JAX**.\n",
    "\n",
    "## Objectif du projet\n",
    "\n",
    "Dans le cadre de ce projet, l‚Äôexp√©rimentation portera sur la librairie **Hugging Face** afin de :\n",
    "- Charger et adapter un mod√®le de type **BERT** √† une t√¢che de classification de textes ;\n",
    "- √âvaluer ses performances sur le dataset **IMDB** ;\n",
    "- Analyser les r√©sultats et discuter des choix r√©alis√©s (mod√®le, preprocessing, param√®tres, etc.).\n",
    "\n",
    "Le travail sera guid√© par les interrogations suivantes :\n",
    "\n",
    "- Bien que tous ces mod√®les reposent sur l'architecture **Transformer**, quelles en sont les sp√©cificit√©s ?\n",
    "- Quel format d'entr√©e est attendu par le mod√®le ?\n",
    "- Quels types de sorties g√©n√®re-t-il ?\n",
    "- Le mod√®le peut-il √™tre utilis√© tel quel ou doit-il √™tre adapt√© √† la t√¢che consid√©r√©e ?\n",
    "\n",
    "Ces questions constituent une part essentielle du travail quotidien d‚Äôun chercheur en NLP et seront examin√©es dans le cadre de ce projet de *fine-tuning*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fi9Qyvavi-Op",
   "metadata": {
    "id": "fi9Qyvavi-Op"
   },
   "source": [
    "## Chargement du jeu d'entra√Ænement\n",
    "\n",
    "La phase initiale consiste √† proc√©der au chargement du jeu d'entra√Ænement au moyen de l'instruction suivante :\n",
    "```python\n",
    "dataset = load_dataset(\"scikit-learn/imdb\", split=\"train\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kMzCkZq8jjLG",
   "metadata": {
    "id": "kMzCkZq8jjLG"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"scikit-learn/imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pvR7eCDkjpso",
   "metadata": {
    "id": "pvR7eCDkjpso"
   },
   "source": [
    "Apr√®s le chargement, il convient de proc√©der √† l'affichage du jeu de donn√©es afin d'en examiner la structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZAqyoLADjsKH",
   "metadata": {
    "id": "ZAqyoLADjsKH"
   },
   "outputs": [],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k9jM4i1WkBQU",
   "metadata": {
    "id": "k9jM4i1WkBQU"
   },
   "source": [
    "L'ex√©cution de la commande `print(dataset)` renvoie la description suivante du jeu de donn√©es :\n",
    "\n",
    "```\n",
    "Dataset({\n",
    "    features: ['review', 'sentiment'],\n",
    "    num_rows: 50000\n",
    "})\n",
    "```\n",
    "\n",
    "Le corpus est constitu√© de 50 000 instances, chacune d√©crite par deux attributs : le texte de la critique (*review*) et son √©tiquette de polarit√© (*sentiment*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dlVDb7LVl60v",
   "metadata": {
    "id": "dlVDb7LVl60v"
   },
   "source": [
    "## Pr√©paration des entr√©es du mod√®le\n",
    "\n",
    "Le format d'entr√©e attendu par **BERT** peut √™tre consid√©r√© comme ¬´ sur-sp√©cifi√© ¬ª, notamment lorsqu'il s'agit de t√¢ches cibl√©es telles que la *sequence classification*, le *word tagging* ou la *paraphrase detection*. Ce format repose sur plusieurs contraintes :\n",
    "\n",
    "* l'ajout de *special tokens* au d√©but et √† la fin de chaque phrase ;\n",
    "* l'application d'un *padding* et d'une troncature afin de ramener toutes les phrases √† une longueur constante ;\n",
    "* la distinction explicite entre *real tokens* et *padding tokens* au moyen de l'*attention mask*.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/auduvignac/llm-finetuning/97e2b676168167ed5ff624f1ad98589c63919d5d/figures/bert_encoding_process.png\" width=\"600\">\n",
    "</p>\n",
    "\n",
    "La figure ci-dessus illustre le processus de pr√©paration et de traitement des s√©quences textuelles dans le mod√®le **BERT**.\n",
    "\n",
    "1. **Tokenisation et ajout de tokens sp√©ciaux**\n",
    "   La s√©quence textuelle est d'abord segment√©e en *tokens*. Deux *special tokens* sont ajout√©s : `[CLS]`, plac√© en d√©but de s√©quence et utilis√© comme repr√©sentation globale, ainsi que `[SEP]`, plac√© en fin de s√©quence et jouant un r√¥le de s√©parateur.\n",
    "\n",
    "2. **Normalisation de la longueur des s√©quences**\n",
    "   Afin d'assurer une longueur uniforme entre les exemples, les s√©quences sont compl√©t√©es par des *\\[PAD] tokens* ou, le cas √©ch√©ant, tronqu√©es √† une taille maximale pr√©d√©finie (*MAX\\_LEN*).\n",
    "\n",
    "3. **Attention mask**\n",
    "   Un vecteur binaire, appel√© *attention mask*, est associ√© √† chaque s√©quence. La valeur `1` indique un *real token* tandis que la valeur `0` correspond √† un *padding token*. Ce m√©canisme permet au mod√®le d'ignorer les positions de remplissage au cours de l'entra√Ænement et de l'inf√©rence.\n",
    "\n",
    "4. **Propagation √† travers les couches du Transformer**\n",
    "   Les repr√©sentations vectorielles des tokens traversent successivement les diff√©rentes couches de l'encodeur Transformer (ici, 12 couches). Chaque couche applique un m√©canisme d'auto-*attention*, permettant de capturer les d√©pendances contextuelles entre tokens.\n",
    "\n",
    "5. **Production de la pr√©diction**\n",
    "   √Ä l'issue de la derni√®re couche, seule la repr√©sentation associ√©e au token `[CLS]` est retenue. Celle-ci est transmise au classificateur, qui produit la pr√©diction finale (par exemple, la polarit√© d'une critique dans une t√¢che de *sentiment analysis*).\n",
    "\n",
    "**Remarque :** Pour des raisons li√©es aux co√ªts de calcul, le mod√®le [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) sera utilis√©. Ce dernier constitue une version r√©duite d'environ 40 % par rapport √† BERT, tout en conservant pr√®s de 95 % des performances du mod√®le original.\n",
    "\n",
    "Afin de pr√©parer les donn√©es textuelles pour le mod√®le, il est n√©cessaire d'instancier un *tokenizer*. Celui-ci a pour r√¥le de segmenter les phrases en unit√©s √©l√©mentaires (*tokens*) et de les convertir en identifiants num√©riques exploitables par le mod√®le. Dans le cadre de ce projet, il sera fait usage du `DistilBertTokenizer` pr√©-entra√Æn√©, correspondant au mod√®le *distilbert-base-uncased*.\n",
    "\n",
    "```python\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased\", do_lower_case=True\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kX8pvscMqN_Q",
   "metadata": {
    "id": "kX8pvscMqN_Q"
   },
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased\", do_lower_case=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ydsx6SAErFMI",
   "metadata": {
    "id": "ydsx6SAErFMI"
   },
   "source": [
    "L'√©tape suivante consiste √† examiner la mani√®re dont le *tokenizer* traite la s√©quence.\n",
    "\n",
    "1. D√©finition d'un message exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BeJ8ydlPsQ1G",
   "metadata": {
    "id": "BeJ8ydlPsQ1G"
   },
   "outputs": [],
   "source": [
    "message = \"hello my name is kevin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o8U3abo0sUje",
   "metadata": {
    "id": "o8U3abo0sUje"
   },
   "source": [
    "2. *Tokenization* de la s√©quence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dSHn-X0QsbS2",
   "metadata": {
    "id": "dSHn-X0QsbS2"
   },
   "outputs": [],
   "source": [
    "tok = tokenizer.tokenize(message)\n",
    "print(\"Tokens dans la s√©quence:\", tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yFtxfZDTsnLN",
   "metadata": {
    "id": "yFtxfZDTsnLN"
   },
   "source": [
    "3. Encodage en identifiants num√©riques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_-tL_02esm30",
   "metadata": {
    "id": "_-tL_02esm30"
   },
   "outputs": [],
   "source": [
    "enc = tokenizer.encode(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnRlPH_3ssEE",
   "metadata": {
    "id": "cnRlPH_3ssEE"
   },
   "source": [
    "4. Mise en correspondance entre token IDs et tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wPSZK1a2sy7G",
   "metadata": {
    "id": "wPSZK1a2sy7G"
   },
   "outputs": [],
   "source": [
    "table = np.array(\n",
    "    [\n",
    "        enc,\n",
    "        [tokenizer.ids_to_tokens[w] for w in enc],\n",
    "    ]\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xElmLOKIs-_Q",
   "metadata": {
    "id": "xElmLOKIs-_Q"
   },
   "source": [
    "5. Affichage des r√©sultats encod√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g_tKu47EtMov",
   "metadata": {
    "id": "g_tKu47EtMov"
   },
   "outputs": [],
   "source": [
    "print(\"Donn√©es d'entr√©e encod√©es:\")\n",
    "print(tabulate(table, headers=[\"Token IDs\", \"Tokens\"], tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_mIUfGOztzYp",
   "metadata": {
    "id": "_mIUfGOztzYp"
   },
   "source": [
    "Le tableau obtenu met en √©vidence la correspondance entre les *tokens* et leurs identifiants num√©riques.\n",
    "Les *tokens* sp√©ciaux `[CLS]` et `[SEP]` encadrent la s√©quence, tandis que chaque mot du texte est associ√© √† un identifiant unique destin√© au traitement par le mod√®le.\n",
    "\n",
    "Les special tokens `[CLS]` et `[SEP]` sont ajout√©s automatiquement par HuggingFace afin de structurer la s√©quence d'entr√©e pour le mod√®le BERT et ses variantes.\n",
    "\n",
    "Le token `[CLS]`, ins√©r√© en d√©but de s√©quence, constitue une repr√©sentation globale de la phrase. Sa sortie est utilis√©e par la couche de classification pour produire la pr√©diction finale (par exemple en *sentiment analysis*).\n",
    "\n",
    "Le token `[SEP]`, plac√© en fin de s√©quence (ou comme s√©parateur entre deux phrases), sert √† marquer les fronti√®res de segments textuels. Il est essentiel dans les t√¢ches n√©cessitant plusieurs entr√©es, comme la comparaison de paires de phrases (*natural language inference*, *paraphrase detection*).\n",
    "\n",
    "Ainsi, ces *tokens* sp√©ciaux garantissent une structuration normalis√©e des entr√©es, indispensable au fonctionnement du mod√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hzd72e1IXuG6",
   "metadata": {
    "id": "hzd72e1IXuG6"
   },
   "source": [
    "## Pr√©traitement des donn√©es\n",
    "\n",
    "Les op√©rations de pr√©traitement appliqu√©es aux donn√©es suivent la m√©thodologie usuelle adopt√©e pour PyTorch et sont analogues √† celles mises en ≈ìuvre pr√©c√©demment.\n",
    "\n",
    "La fonction `preprocessing_fn` d√©finit les op√©rations de pr√©paration appliqu√©es √† chaque exemple du corpus.\n",
    "Le champ `review` est converti en une s√©quence d'identifiants num√©riques au moyen du *tokenizer*. L'encodage est effectu√© sans ajout de *special tokens*, avec troncature √† une longueur maximale de 256 et sans application de *padding* ni g√©n√©ration de *attention mask*.\n",
    "Par ailleurs, l'√©tiquette `sentiment` est transform√©e en valeur binaire : `0` pour les exemples n√©gatifs et `1` pour les exemples positifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r9pji8nHX7l0",
   "metadata": {
    "id": "r9pji8nHX7l0"
   },
   "outputs": [],
   "source": [
    "def preprocessing_fn(x, tokenizer):\n",
    "    \"\"\"\n",
    "    Pr√©traite un exemple du corpus en encodant le texte et en transformant\n",
    "    l'√©tiquette.\n",
    "\n",
    "    Param√®tres\n",
    "    ----------\n",
    "    x : dict\n",
    "        Exemple du corpus contenant au moins deux cl√©s :\n",
    "        - \"review\" (str) : le texte √† encoder,\n",
    "        - \"sentiment\" (str) : √©tiquette textuelle (\"negative\" ou \"positive\").\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        Tokenizer Hugging Face utilis√© pour convertir le texte en identifiants num√©riques.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    dict\n",
    "        Dictionnaire enrichi contenant :\n",
    "        - \"input_ids\" (List[int]) : s√©quence encod√©e de tokens num√©riques,\n",
    "        - \"labels\" (int) : √©tiquette binaire (0 = n√©gatif, 1 = positif).\n",
    "    \"\"\"\n",
    "    x[\"input_ids\"] = tokenizer.encode(\n",
    "        x[\"review\"],\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=False,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    x[\"labels\"] = 0 if x[\"sentiment\"] == \"negative\" else 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lm-7hjbtbv9-",
   "metadata": {
    "id": "lm-7hjbtbv9-"
   },
   "outputs": [],
   "source": [
    "n_samples = 2000  # nombre d'exemples d'entra√Ænement\n",
    "\n",
    "# op√©ration de shuffle sur les donn√©es\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# Selection de 2000 √©chantillons\n",
    "splitted_dataset = dataset.select(range(n_samples))\n",
    "\n",
    "# Tokenization du dataset\n",
    "splitted_dataset = splitted_dataset.map(\n",
    "    preprocessing_fn, fn_kwargs={\"tokenizer\": tokenizer}\n",
    ")\n",
    "\n",
    "\n",
    "# Suppression des colonnes inutiles\n",
    "splitted_dataset = splitted_dataset.select_columns([\"input_ids\", \"labels\"])\n",
    "\n",
    "# S√©paration en donn√©es d'entra√Ænement et de validation\n",
    "splitted_dataset = splitted_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "train_set = splitted_dataset[\"train\"]\n",
    "valid_set = splitted_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BlD6Hu4Pc6AR",
   "metadata": {
    "id": "BlD6Hu4Pc6AR"
   },
   "outputs": [],
   "source": [
    "class DataCollator:\n",
    "    \"\"\"\n",
    "    A data collator that pads input batches to a maximum length of 256 tokens.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: The tokenizer to use for padding the input batch.\n",
    "\n",
    "    Returns:\n",
    "        Padded batch with tokenized inputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        \"\"\"\n",
    "        Initializes the DataCollator with the given tokenizer.\n",
    "\n",
    "        Args:\n",
    "            tokenizer: The tokenizer to use for padding the input batch.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        Pads the input batch to a maximum length of 256 tokens using the\n",
    "        provided tokenizer.\n",
    "\n",
    "        Args:\n",
    "            batch: The input batch to be padded.\n",
    "\n",
    "        Returns:\n",
    "            Padded batch with tokenized inputs.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.pad(\n",
    "            batch, padding=\"longest\", max_length=256, return_tensors=\"pt\"\n",
    "        )\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3hgHBwEEdOjr",
   "metadata": {
    "id": "3hgHBwEEdOjr"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "om_6jieEdRLh",
   "metadata": {
    "id": "om_6jieEdRLh"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_set, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_set, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "n_valid = len(valid_set)\n",
    "n_train = len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ln_5eYacwqtG",
   "metadata": {
    "id": "Ln_5eYacwqtG"
   },
   "source": [
    "Pour cette t√¢che, l'entra√Ænement sera effectu√© √† partir d'un mod√®le initialis√© al√©atoirement.\n",
    "\n",
    "R√©cup√©ration de la configuration de l'architecture\n",
    "\n",
    "Dans la biblioth√®que Hugging Face, les param√®tres du mod√®le sont sp√©cifi√©s par l'interm√©diaire d'un fichier de configuration.\n",
    "\n",
    "La configuration du mod√®le peut √™tre r√©cup√©r√©e gr√¢ce au code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fil8hNEkxoCq",
   "metadata": {
    "id": "fil8hNEkxoCq"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertConfig\n",
    "\n",
    "model_config = DistilBertConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(model_config)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
