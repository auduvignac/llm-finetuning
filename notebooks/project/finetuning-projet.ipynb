{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "g3PmJq6pQqbP",
   "metadata": {
    "id": "g3PmJq6pQqbP"
   },
   "source": [
    "# *Fine-tuning* de *Large Language Models* (LLMs)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"https://colab.research.google.com/github/auduvignac/llm-finetuning/blob/main/notebooks/project/finetuning-projet.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Ouvrir dans Google Colab\"/>\n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "Le but de ce projet est de réaliser le *fine-tuning* d'un LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9-t5uVFuNQiF",
   "metadata": {
    "id": "9-t5uVFuNQiF"
   },
   "source": [
    "## Installation des bibliothèques/libraires requises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-wuPJgZriD29",
   "metadata": {
    "id": "-wuPJgZriD29",
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!wget -q https://raw.githubusercontent.com/auduvignac/llm-finetuning/refs/heads/main/setup_env.py -O setup_env.py\n",
    "%run setup_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nCALzgA2Nleg",
   "metadata": {
    "id": "nCALzgA2Nleg"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "\n",
    "import contextlib\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from tabulate import (\n",
    "    tabulate,\n",
    ")\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    ")\n",
    "from tqdm import (\n",
    "    tqdm,\n",
    ")\n",
    "\n",
    "# Si le notebook est exécuté dans un environnement jupyter, la librairie\n",
    "# ci-dessus peut être utilisée\n",
    "from tqdm.notebook import (\n",
    "    tqdm,\n",
    ")\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DistilBertConfig,\n",
    "    DistilBertModel,\n",
    "    DistilBertTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# Utilisation d’un GPU avec CUDA lorsque disponible sur la machine d’exécution.\n",
    "# L’utilisation d’un GPU pour l’apprentissage entraîne souvent d’énormes\n",
    "# accélérations lors de l’entraînement.\n",
    "# Voir https://developer.nvidia.com/cuda-downloads pour installer CUDA\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YX7e3F_BR0BU",
   "metadata": {
    "id": "YX7e3F_BR0BU"
   },
   "outputs": [],
   "source": [
    "class LlamaFineTuner:\n",
    "    def __init__(\n",
    "        self, model_id=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "    ):\n",
    "        self.model_id = model_id\n",
    "\n",
    "        # Config quantization 4-bit\n",
    "        self.bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=\"float16\",\n",
    "            bnb_4bit_use_double_quant=False,\n",
    "        )\n",
    "\n",
    "        # Chargement du tokenizer et modèle\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_id,\n",
    "            quantization_config=self.bnb_config,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        self.dataset = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.model)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        max_new_tokens: int = 200,\n",
    "        temperature: float = 1.0,\n",
    "        instruction_mode: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Génèreation de texte avec le modèle.\n",
    "\n",
    "        Si instruction_mode=True, formate le prompt en mode\n",
    "        Instruction/Response.\n",
    "        \"\"\"\n",
    "\n",
    "        DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Formatage du prompt\n",
    "        if instruction_mode:\n",
    "            text = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
    "        else:\n",
    "            text = prompt\n",
    "\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "        # Correction du warning: désactivation du checkpointing et\n",
    "        # réactivation du cache\n",
    "        with contextlib.suppress(Exception):\n",
    "            self.model.gradient_checkpointing_disable()\n",
    "            self.model.config.use_cache = True\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "        )\n",
    "        return self.tokenizer.decode(outputs[0])\n",
    "\n",
    "    def prepare_dataset(self, dataset_name=\"tatsu-lab/alpaca\"):\n",
    "        def process_data(sample):\n",
    "            return self.tokenizer(sample[\"text\"])\n",
    "\n",
    "        data = load_dataset(dataset_name)\n",
    "        self.dataset = data.map(process_data, batched=True)\n",
    "        return self.dataset\n",
    "\n",
    "    def prepare_for_kbit_training(self):\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "\n",
    "    def print_trainable_parameters(self):\n",
    "        trainable_params = 0\n",
    "        all_param = 0\n",
    "        for _, param in self.model.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            if param.requires_grad:\n",
    "                trainable_params += param.numel()\n",
    "        print(\n",
    "            f\"trainable params: {trainable_params} || all params: {all_param} \"\n",
    "            f\"|| trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "        )\n",
    "\n",
    "    def apply_lora(self):\n",
    "        config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, config)\n",
    "        self.print_trainable_parameters()\n",
    "\n",
    "    def train(self, output_dir=\"outputs\", max_steps=100):\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            per_device_train_batch_size=16,\n",
    "            gradient_accumulation_steps=1,\n",
    "            max_steps=max_steps,\n",
    "            learning_rate=2e-4,\n",
    "            fp16=True,\n",
    "            logging_steps=1,\n",
    "            output_dir=output_dir,\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            report_to=\"none\",\n",
    "        )\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            train_dataset=self.dataset[\"train\"],\n",
    "            args=training_args,\n",
    "            data_collator=DataCollatorForLanguageModeling(\n",
    "                self.tokenizer, mlm=False\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.model.config.use_cache = False\n",
    "        self.trainer.train()\n",
    "\n",
    "    def workflow(\n",
    "        self,\n",
    "        quick_test_prompt: str = \"Paris is the capital of\",\n",
    "        instruction_prompt: str = \"Propose an outdoor activity.\",\n",
    "        max_steps: int = 10,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Exécution d'un workflow complet :\n",
    "          1. Génération initiale (avant entraînement)\n",
    "          2. Préparation du dataset\n",
    "          3. Préparation du modèle pour k-bit training\n",
    "          4. Application de LoRA\n",
    "          5. Entraînement (max_steps configurables)\n",
    "          6. Génération finale en mode instruction\n",
    "\n",
    "        Args:\n",
    "            quick_test_prompt: Texte de génération avant entraînement\n",
    "            instruction_prompt: Instruction pour la génération finale\n",
    "            max_steps: Nombre d'étapes d'entraînement\n",
    "        \"\"\"\n",
    "        print(\" Génération initiale (avant fine-tuning)…\")\n",
    "        print(self.generate(quick_test_prompt, max_new_tokens=50))\n",
    "\n",
    "        print(\"\\n Préparation du dataset…\")\n",
    "        self.prepare_dataset()\n",
    "\n",
    "        print(\"\\n Préparation k-bit training…\")\n",
    "        self.prepare_for_kbit_training()\n",
    "\n",
    "        print(\"\\n Application de LoRA…\")\n",
    "        self.apply_lora()\n",
    "\n",
    "        print(\"\\n Entraînement…\")\n",
    "        self.train(max_steps=max_steps)\n",
    "\n",
    "        print(\"\\nGénération finale (après fine-tuning)…\")\n",
    "        print(\n",
    "            self.generate(\n",
    "                instruction_prompt, instruction_mode=True, max_new_tokens=100\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CONQSfgg_tPZ",
   "metadata": {
    "id": "CONQSfgg_tPZ"
   },
   "outputs": [],
   "source": [
    "llm = LlamaFineTuner()\n",
    "llm.workflow()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
