{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "g3PmJq6pQqbP",
      "metadata": {
        "id": "g3PmJq6pQqbP"
      },
      "source": [
        "# *Fine-tuning* de *Large Language Models* (LLMs)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <a href=\"https://colab.research.google.com/github/auduvignac/llm-finetuning/blob/main/notebooks/project/finetuning-projet.ipynb\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Ouvrir dans Google Colab\"/>\n",
        "  </a>\n",
        "</p>\n",
        "\n",
        "Le but de ce projet est de r√©aliser le *fine-tuning* d'un LLM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9-t5uVFuNQiF",
      "metadata": {
        "id": "9-t5uVFuNQiF"
      },
      "source": [
        "## Installation des biblioth√®ques/libraires requises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-wuPJgZriD29",
      "metadata": {
        "id": "-wuPJgZriD29",
        "vscode": {
          "languageId": "powershell"
        }
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/auduvignac/llm-finetuning/refs/heads/main/setup_env.py -O setup_env.py\n",
        "%run setup_env.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nCALzgA2Nleg",
      "metadata": {
        "id": "nCALzgA2Nleg"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_formats = ['svg']\n",
        "\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "from datasets import (\n",
        "    load_dataset,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        ")\n",
        "from tabulate import (\n",
        "    tabulate,\n",
        ")\n",
        "from torch.utils.data import (\n",
        "    DataLoader,\n",
        ")\n",
        "from tqdm import (\n",
        "    tqdm,\n",
        ")\n",
        "\n",
        "# Si le notebook est ex√©cut√© dans un environnement jupyter, la librairie\n",
        "# ci-dessus peut √™tre utilis√©e\n",
        "from tqdm.notebook import (\n",
        "    tqdm,\n",
        ")\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    DistilBertConfig,\n",
        "    DistilBertModel,\n",
        "    DistilBertTokenizer,\n",
        ")\n",
        "\n",
        "# Utilisation d‚Äôun GPU avec CUDA lorsque disponible sur la machine d‚Äôex√©cution.\n",
        "# L‚Äôutilisation d‚Äôun GPU pour l‚Äôapprentissage entra√Æne souvent d‚Äô√©normes\n",
        "# acc√©l√©rations lors de l‚Äôentra√Ænement.\n",
        "# Voir https://developer.nvidia.com/cuda-downloads pour installer CUDA\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YX7e3F_BR0BU",
      "metadata": {
        "id": "YX7e3F_BR0BU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import (\n",
        "    prepare_model_for_kbit_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model\n",
        ")\n",
        "\n",
        "\n",
        "class LlamaFineTuner:\n",
        "    def __init__(self, model_id=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"):\n",
        "        self.model_id = model_id\n",
        "\n",
        "        # Config quantization 4-bit\n",
        "        self.bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=\"float16\",\n",
        "            bnb_4bit_use_double_quant=False,\n",
        "        )\n",
        "\n",
        "        # Chargement du tokenizer et mod√®le\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_id,\n",
        "            quantization_config=self.bnb_config,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        self.dataset = None\n",
        "        self.trainer = None\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.model)\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_new_tokens: int = 200,\n",
        "        temperature: float = 1.0,\n",
        "        instruction_mode: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        G√©n√®re du texte avec le mod√®le.\n",
        "        Si instruction_mode=True, formate le prompt en mode Instruction/Response.\n",
        "        \"\"\"\n",
        "\n",
        "        DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Formatage du prompt\n",
        "        if instruction_mode:\n",
        "            text = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "        else:\n",
        "            text = prompt\n",
        "\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "        # üîß Corrige le warning: on d√©sactive checkpointing et r√©active le cache\n",
        "        try:\n",
        "            self.model.gradient_checkpointing_disable()\n",
        "            self.model.config.use_cache = True\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True,\n",
        "        )\n",
        "        return self.tokenizer.decode(outputs[0])\n",
        "\n",
        "    def prepare_dataset(self, dataset_name=\"tatsu-lab/alpaca\"):\n",
        "        def process_data(sample):\n",
        "            return self.tokenizer(sample[\"text\"])\n",
        "\n",
        "        data = load_dataset(dataset_name)\n",
        "        self.dataset = data.map(process_data, batched=True)\n",
        "        return self.dataset\n",
        "\n",
        "    def prepare_for_kbit_training(self):\n",
        "        self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "    def print_trainable_parameters(self):\n",
        "        trainable_params = 0\n",
        "        all_param = 0\n",
        "        for _, param in self.model.named_parameters():\n",
        "            all_param += param.numel()\n",
        "            if param.requires_grad:\n",
        "                trainable_params += param.numel()\n",
        "        print(\n",
        "            f\"trainable params: {trainable_params} || all params: {all_param} \"\n",
        "            f\"|| trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "        )\n",
        "\n",
        "    def apply_lora(self):\n",
        "        config = LoraConfig(\n",
        "            r=8,\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "            lora_dropout=0.05,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\"\n",
        "        )\n",
        "        self.model = get_peft_model(self.model, config)\n",
        "        self.print_trainable_parameters()\n",
        "\n",
        "    def train(self, output_dir=\"outputs\", max_steps=100):\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            per_device_train_batch_size=16,\n",
        "            gradient_accumulation_steps=1,\n",
        "            max_steps=max_steps,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=True,\n",
        "            logging_steps=1,\n",
        "            output_dir=output_dir,\n",
        "            optim=\"paged_adamw_8bit\",\n",
        "            report_to=\"none\"\n",
        "        )\n",
        "\n",
        "        self.trainer = Trainer(\n",
        "            model=self.model,\n",
        "            train_dataset=self.dataset[\"train\"],\n",
        "            args=training_args,\n",
        "            data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False),\n",
        "        )\n",
        "\n",
        "        self.model.config.use_cache = False\n",
        "        self.trainer.train()\n",
        "\n",
        "    def workflow(\n",
        "        self,\n",
        "        quick_test_prompt: str = \"Paris is the capital of\",\n",
        "        instruction_prompt: str = \"Propose an outdoor activity.\",\n",
        "        max_steps: int = 10,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Ex√©cute un workflow complet :\n",
        "          1. G√©n√©ration initiale (avant entra√Ænement)\n",
        "          2. Pr√©paration du dataset\n",
        "          3. Pr√©paration du mod√®le pour k-bit training\n",
        "          4. Application de LoRA\n",
        "          5. Entra√Ænement (max_steps configurables)\n",
        "          6. G√©n√©ration finale en mode instruction\n",
        "\n",
        "        Args:\n",
        "            quick_test_prompt: Texte de g√©n√©ration avant entra√Ænement\n",
        "            instruction_prompt: Instruction pour la g√©n√©ration finale\n",
        "            max_steps: Nombre d‚Äô√©tapes d‚Äôentra√Ænement\n",
        "        \"\"\"\n",
        "        print(\" G√©n√©ration initiale (avant fine-tuning)‚Ä¶\")\n",
        "        print(self.generate(quick_test_prompt, max_new_tokens=50))\n",
        "\n",
        "        print(\"\\n Pr√©paration du dataset‚Ä¶\")\n",
        "        self.prepare_dataset()\n",
        "\n",
        "        print(\"\\n Pr√©paration k-bit training‚Ä¶\")\n",
        "        self.prepare_for_kbit_training()\n",
        "\n",
        "        print(\"\\n Application de LoRA‚Ä¶\")\n",
        "        self.apply_lora()\n",
        "\n",
        "        print(\"\\n Entra√Ænement‚Ä¶\")\n",
        "        self.train(max_steps=max_steps)\n",
        "\n",
        "        print(\"\\nG√©n√©ration finale (apr√®s fine-tuning)‚Ä¶\")\n",
        "        print(self.generate(instruction_prompt, instruction_mode=True, max_new_tokens=100))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LlamaFineTuner()\n",
        "llm.workflow()"
      ],
      "metadata": {
        "id": "CONQSfgg_tPZ"
      },
      "id": "CONQSfgg_tPZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}