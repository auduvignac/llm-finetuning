{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "66150ed9",
      "metadata": {
        "id": "66150ed9"
      },
      "source": [
        "# Classification de textes avec les Transformers\n",
        "\n",
        "<p align=\"center\">\n",
        "  <a href=\"https://raw.githubusercontent.com/auduvignac/llm-finetuning/refs/heads/main/notebooks/project/finetuning-projet.ipynb\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Ouvrir dans Google Colab\"/>\n",
        "  </a>\n",
        "</p>\n",
        "\n",
        "Ce projet porte sur une tâche de **classification de textes** appliquée au **jeu de données IMDB** (analyse de sentiments ou *sentimental analysis*).  \n",
        "Ce travail s'appuiera sur les **architectures de type encodeur**, en particulier l’un des modèles les plus connus : **BERT** (et sa variante légère **DistilBERT**).\n",
        "\n",
        "## Présentation\n",
        "\n",
        "Le projet consiste à utiliser la librairie `datasets` pour le chargement des données et les `tokenizers` pour le prétraitement des textes.\n",
        "\n",
        "La librairie **Transformers** propose une API simple pour utiliser des modèles pré-entraînés tels que **BERT** ou **GPT**. Elle facilite leur téléchargement, leur ré-entraînement et leur intégration, tout en réduisant les coûts de calcul et en restant compatible avec **PyTorch, TensorFlow et JAX**.\n",
        "\n",
        "## Objectif du projet\n",
        "\n",
        "Dans le cadre de ce projet, l’expérimentation portera sur la librairie **Hugging Face** afin de :\n",
        "- Charger et adapter un modèle de type **BERT** à une tâche de classification de textes ;\n",
        "- Évaluer ses performances sur le dataset **IMDB** ;\n",
        "- Analyser les résultats et discuter des choix réalisés (modèle, preprocessing, paramètres, etc.).\n",
        "\n",
        "Le travail sera guidé par les interrogations suivantes :\n",
        "\n",
        "- Bien que tous ces modèles reposent sur l’architecture **Transformer**, quelles en sont les spécificités ?\n",
        "- Quel format d’entrée est attendu par le modèle ?\n",
        "- Quels types de sorties génère-t-il ?\n",
        "- Le modèle peut-il être utilisé tel quel ou doit-il être adapté à la tâche considérée ?\n",
        "\n",
        "Ces questions constituent une part essentielle du travail quotidien d’un chercheur en NLP et seront examinées dans le cadre de ce projet de *fine-tuning*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "powershell"
        },
        "id": "-wuPJgZriD29",
        "outputId": "b38f4840-fde4-4533-f05c-21f79a6588cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚡ Exécution sur Colab : vérification des dépendances...\n",
            "✅ datasets déjà présent.\n",
            "✅ matplotlib déjà présent.\n",
            "✅ numpy déjà présent.\n",
            "✅ tabulate déjà présent.\n",
            "✅ torch déjà présent.\n",
            "✅ tqdm déjà présent.\n",
            "✅ transformers déjà présent.\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://raw.githubusercontent.com/auduvignac/llm-finetuning/refs/heads/main/setup_env.py -O setup_env.py\n",
        "%run setup_env.py"
      ],
      "id": "-wuPJgZriD29"
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_formats = ['svg']\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from transformers import DistilBertTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "from tabulate import tabulate\n",
        "from datasets import load_dataset\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# If the machine you run this on has a GPU available with CUDA installed,\n",
        "# use it. Using a GPU for learning often leads to huge speedups in training.\n",
        "# See https://developer.nvidia.com/cuda-downloads for installing CUDA\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ],
      "metadata": {
        "id": "v68joACNiUZY",
        "outputId": "458062bd-413a-40c5-c5a5-f01c43af852d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "v68joACNiUZY",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}