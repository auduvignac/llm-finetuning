{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5947a1",
   "metadata": {},
   "source": [
    "# Text classification using Transformers.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <a href=\"https://colab.research.google.com/github/auduvignac/llm-finetuning/blob/main/notebooks/project/draft.ipynb\" target=\"_blank\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Ouvrir dans Google Colab\"/>\n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "This lab will focus on text classification on the Imdb dataset.\n",
    "In this lab session, we will focus on encoder-based transformer architecture, through the lens of the most famous model: **BERT**.\n",
    "\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "## HuggingFace\n",
    "\n",
    "We have already experimented with some components provided by the HuggingFace library:\n",
    "- the `datasets` library,\n",
    "- the `tokenizer`.\n",
    "\n",
    "Actually, HuggingFace library provides convenient API to deal with transformer models, like BERT, GPT, etc.  To quote their website: *Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. Transformers support framework interoperability between PyTorch, TensorFlow, and JAX.*\n",
    "\n",
    "## Goal of the lab session\n",
    "\n",
    "We will experiment with the HuggingFace library. You'll have to load a model and to run it on your task.\n",
    "\n",
    "Important things to keep in in minds are:\n",
    "- Even if each model is a Transformer, they all have their peculiarities.\n",
    "- What is the exact input format expected by the model?\n",
    "- What is its exact output?\n",
    "- Can you use the available model as is or should you make some modifications for your task?\n",
    "\n",
    "These questions are actually part of the life of a NLP scientist. We will adress some of these questions in this lab and in the next lessons / labs / HW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7060af9",
   "metadata": {},
   "source": [
    "## Libraries import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe74cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import (\n",
    "    DatasetDict,\n",
    "    load_dataset,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    brier_score_loss,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    ")\n",
    "from tabulate import tabulate\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# If the machine you run this on has a GPU available with CUDA installed,\n",
    "# use it. Using a GPU for learning often leads to huge speedups in training.\n",
    "# See https://developer.nvidia.com/cuda-downloads for installing CUDA\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1d063f",
   "metadata": {},
   "source": [
    "## Device set up & reproducibility\n",
    "\n",
    "- Ensures results are reproducible across runs (same seed = same shuffling, same weight initialization, etc.).\n",
    "- Chooses GPU if available, otherwise falls back to CPU.\n",
    "- Printing confirms where your model will run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b01fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8796aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collators collect and organise information.\n",
    "# They clean data and assure the forms of data from a variety of sources,\n",
    "# including: primary data. survey data.\n",
    "class DataCollator:\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(\n",
    "        self, batch, max_length=256, padding=\"longest\", return_tensors=\"pt\"\n",
    "    ):\n",
    "        return self.tokenizer.pad(\n",
    "            batch,\n",
    "            padding=padding,\n",
    "            max_length=max_length,\n",
    "            return_tensors=return_tensors,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ab9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMFineTuner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset=\"scikit-learn/imdb\",\n",
    "        model_cls=DistilBertForSequenceClassification,\n",
    "        num_labels=2,  # \"negative\" (0) and \"positive\" (1)\n",
    "        pretrained_model_name_or_path=\"distilbert-base-uncased\",\n",
    "        tokenizer_cls=DistilBertTokenizer,\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "        self.data_collator = None\n",
    "        self.train_set = None\n",
    "        self.valid_set = None\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.model_cls = model_cls\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_model_name_or_path = pretrained_model_name_or_path\n",
    "        self.tokenizer_cls = tokenizer_cls\n",
    "\n",
    "    def set_dataset(self, verbose=False):\n",
    "        self.dataset = load_dataset(self.dataset)\n",
    "        if verbose:\n",
    "            print(f\"Dataset loaded :\\n\"\n",
    "                  f\"{self.dataset}\\n\"\n",
    "                  f\"with {len(self.dataset)} examples.\"\n",
    "            )\n",
    "\n",
    "    def set_tokenizer(\n",
    "        self,\n",
    "        do_lower_case=True,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.tokenizer = self.tokenizer_cls.from_pretrained(\n",
    "            self.pretrained_model_name_or_path, do_lower_case=do_lower_case\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Tokenizer {self.tokenizer_cls.__name__} loaded from \"\n",
    "                f\"{self.pretrained_model_name_or_path}\"\n",
    "            )\n",
    "\n",
    "    def set_data_collator(self):\n",
    "        self.data_collator = DataCollator(self.tokenizer)\n",
    "\n",
    "    def split_dataset(\n",
    "        self, max_length=256, n_samples=2000, seed=42, test_size=0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Prepares the dataset for training:\n",
    "\n",
    "        - Shuffles and selects a subset\n",
    "        - Tokenizes the reviews and generates input_ids and labels\n",
    "        - Removes unnecessary columns\n",
    "        - Splits into train/validation sets\n",
    "\n",
    "        Args:\n",
    "            n_samples (int): number of examples to select\n",
    "            test_size (float): proportion of the data to use for validation\n",
    "            max_length (int): maximum sequence length (truncate if longer)\n",
    "        \"\"\"\n",
    "\n",
    "        def preprocessing_fn(x, tokenizer):\n",
    "            # Convertit le texte en IDs de tokens\n",
    "            x[\"input_ids\"] = tokenizer.encode(\n",
    "                x[\"review\"],\n",
    "                add_special_tokens=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding=False,\n",
    "                return_attention_mask=False,\n",
    "            )\n",
    "            # Encode le label\n",
    "            x[\"labels\"] = 0 if x[\"sentiment\"] == \"negative\" else 1\n",
    "            return x\n",
    "\n",
    "        # Mélanger et sous-échantillonner\n",
    "        dataset = self.dataset[\"train\"].shuffle(seed).select(range(n_samples))\n",
    "\n",
    "        # Appliquer le prétraitement\n",
    "        dataset = dataset.map(\n",
    "            preprocessing_fn, fn_kwargs={\"tokenizer\": self.tokenizer}\n",
    "        )\n",
    "\n",
    "        # Garder uniquement les colonnes utiles\n",
    "        dataset = dataset.select_columns([\"input_ids\", \"labels\"])\n",
    "\n",
    "        # Split train / validation\n",
    "        splitted = dataset.train_test_split(test_size=test_size)\n",
    "\n",
    "        self.train_set = splitted[\"train\"]\n",
    "        self.valid_set = splitted[\"test\"]\n",
    "\n",
    "    def set_loaders(self, train_batch_size=4, eval_batch_size=4):\n",
    "        if not self.data_collator:\n",
    "            raise ValueError(\"Data collator must be set before data loaders.\")\n",
    "        if not self.train_set or not self.valid_set:\n",
    "            raise ValueError(\"Dataset must be split before data loaders.\")\n",
    "        self.train_loader = DataLoader(\n",
    "            batch_size=train_batch_size,\n",
    "            collate_fn=self.data_collator,\n",
    "            dataset=self.train_set,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        self.valid_loader = DataLoader(\n",
    "            batch_size=eval_batch_size,\n",
    "            collate_fn=self.data_collator,\n",
    "            dataset=self.valid_set,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        self.n_valid = len(self.valid_set)\n",
    "        self.n_train = len(self.train_set)\n",
    "\n",
    "    def set_model(\n",
    "        self,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        model = self.model_cls.from_pretrained(\n",
    "            pretrained_model_name_or_path=self.pretrained_model_name_or_path,\n",
    "            num_labels=self.num_labels,\n",
    "        )\n",
    "        self.model = model.to(self.device)\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Model {self.model_cls.__name__} loaded with \"\n",
    "                f\"{self.model.num_labels} labels.\"\n",
    "            )\n",
    "\n",
    "    def set_optimizer(\n",
    "        self,\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "    ):\n",
    "        # AdamW is a variant of Adam that includes weight decay\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "    def set_scheduler(\n",
    "        self,\n",
    "        num_epochs=3,\n",
    "    ):\n",
    "        if not self.optimizer:\n",
    "            raise ValueError(\"Optimizer must be set before the scheduler.\")\n",
    "        if not self.train_loader:\n",
    "            raise ValueError(\"Data loaders must be set before scheduler.\")\n",
    "        # Total training steps = number of batches * number of epochs\n",
    "        self.num_total_steps = len(self.train_loader) * num_epochs\n",
    "        self.num_warmup_steps = int(0.1 * self.num_total_steps)\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=self.num_warmup_steps,\n",
    "            num_training_steps=self.num_total_steps,\n",
    "        )\n",
    "\n",
    "    def set_optimizer_and_scheduler(\n",
    "        self,\n",
    "        learning_rate=5e-5,\n",
    "        num_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        verbose=False,\n",
    "    ):\n",
    "        self.set_optimizer(learning_rate, weight_decay)\n",
    "        self.set_scheduler(num_epochs)\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Optimizer and scheduler set with {self.num_total_steps} \"\n",
    "                f\"training steps and {self.num_warmup_steps} warmup steps.\"\n",
    "            )\n",
    "\n",
    "    def train_and_validate(\n",
    "        self, epochs=3, max_grad_norm=1.0, save_dir=\"./distilbert-best\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Trains and validates the model for a given number of epochs.\n",
    "\n",
    "        - Performs forward/backward passes with gradient clipping\n",
    "        - Updates optimizer and scheduler\n",
    "        - Evaluates on validation set at the end of each epoch\n",
    "        - Saves the best model checkpoint based on validation loss\n",
    "\n",
    "        Args:\n",
    "            epochs (int): number of epochs to train\n",
    "            max_grad_norm (float): gradient clipping norm\n",
    "            save_dir (str): directory to save the best model\n",
    "        \"\"\"\n",
    "        best_val_loss = float(\"inf\")\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "\n",
    "            # -------- TRAIN --------\n",
    "            total_train_loss = 0.0\n",
    "            self.model.train()\n",
    "            for batch in self.train_loader:\n",
    "                # move batch tensors to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "\n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                # forward pass (returns loss when 'labels' is provided)\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "                # backward pass\n",
    "                loss.backward()\n",
    "\n",
    "                # gradient clipping\n",
    "                clip_grad_norm_(\n",
    "                    self.model.parameters(), max_norm=max_grad_norm\n",
    "                )\n",
    "\n",
    "                # optimizer + scheduler step\n",
    "                self.optimizer.step()\n",
    "                self.scheduler.step()\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(self.train_loader)\n",
    "            print(f\"  Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            # -------- VALIDATE --------\n",
    "            self.model.eval()\n",
    "            total_val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in self.valid_loader:\n",
    "                    batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "\n",
    "                    outputs = self.model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                    logits = outputs.logits\n",
    "\n",
    "                    total_val_loss += loss.item()\n",
    "\n",
    "                    preds = logits.argmax(dim=-1)\n",
    "                    correct += (preds == batch[\"labels\"]).sum().item()\n",
    "                    total += batch[\"labels\"].size(0)\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(self.valid_loader)\n",
    "            val_acc = correct / total if total > 0 else 0.0\n",
    "            print(\n",
    "                f\"  Validation loss: {avg_val_loss:.4f} | Accuracy: {val_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "            # save best checkpoint\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                self.model.save_pretrained(save_dir)\n",
    "                self.tokenizer.save_pretrained(save_dir)\n",
    "                print(f\"Saved new best model to {save_dir}\")\n",
    "\n",
    "    def predict_sentiment(self, text):\n",
    "        # Tokenize the input\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            padding=\"max_length\",  # pad single example\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "\n",
    "        # Get predicted label\n",
    "        pred_label = int(logits.argmax(dim=-1).cpu().item())\n",
    "        label_str = \"positive\" if pred_label == 1 else \"negative\"\n",
    "\n",
    "        return {\n",
    "            \"text\": text,\n",
    "            \"pred_label\": label_str,\n",
    "            \"probabilities\": {\n",
    "                \"negative\": float(probs[0]),\n",
    "                \"positive\": float(probs[1]),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def predict_batch(self, texts, max_length=256):\n",
    "        \"\"\"\n",
    "        Predict sentiment for a batch of texts (list of strings).\n",
    "        Returns a list of dicts with labels and probabilities.\n",
    "        \"\"\"\n",
    "        # Tokenize the whole batch at once\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=True,  # pad to longest in batch\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "        # Decode predictions\n",
    "        results = []\n",
    "        for text, prob in zip(texts, probs):\n",
    "            pred_label = int(prob.argmax())\n",
    "            label_str = \"positive\" if pred_label == 1 else \"negative\"\n",
    "            results.append(\n",
    "                {\n",
    "                    \"text\": text,\n",
    "                    \"pred_label\": label_str,\n",
    "                    \"probabilities\": {\n",
    "                        \"negative\": float(prob[0]),\n",
    "                        \"positive\": float(prob[1]),\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    def count_parameters(self):\n",
    "        total = sum(p.numel() for p in self.model.parameters())\n",
    "        trainable = sum(\n",
    "            p.numel() for p in self.model.parameters() if p.requires_grad\n",
    "        )\n",
    "        return total, trainable\n",
    "\n",
    "    def evaluate(self):\n",
    "      if not self.valid_loader:\n",
    "          raise ValueError(\"Data loaders must be set before evaluation.\")\n",
    "      if not self.model:\n",
    "          raise ValueError(\"Model must be set before evaluation.\")\n",
    "      self.model.eval()\n",
    "      all_preds = []\n",
    "      all_labels = []\n",
    "      with torch.no_grad():\n",
    "          for batch in self.valid_loader:\n",
    "              batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "              outputs = self.model(**batch)\n",
    "              logits = outputs.logits\n",
    "              preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "              labels = batch[\"labels\"].cpu().numpy()\n",
    "              all_preds.extend(preds)\n",
    "              all_labels.extend(labels)\n",
    "      accuracy = accuracy_score(all_labels, all_preds)\n",
    "      cm = confusion_matrix(all_labels, all_preds)\n",
    "      f1 = f1_score(all_labels, all_preds)\n",
    "      report = classification_report(all_labels, all_preds, target_names=[\"negative\", \"positive\"])\n",
    "      return {\n",
    "          \"accuracy\": accuracy,\n",
    "          \"classification_report\": report,\n",
    "          \"confusion_matrix\": cm,\n",
    "          \"f1_score\": f1,\n",
    "      }\n",
    "\n",
    "    def workflow(self, mode=\"train\", verbose=False):\n",
    "        \"\"\"\n",
    "        Run the full workflow depending on mode.\n",
    "\n",
    "        Args:\n",
    "            mode (str): \"train\" (fine-tuning from scratch) or \"inference\" (load pretrained model)\n",
    "            model_path (str): path to a saved model if mode=\"inference\"\n",
    "        \"\"\"\n",
    "        if mode == \"train\":\n",
    "            print(\"Starting training workflow...\")\n",
    "            # 1) We load the dataset\n",
    "            self.set_dataset(verbose=verbose)\n",
    "            # 2) We load tokenizer + model (for binary classification)\n",
    "            self.set_tokenizer(verbose=verbose)\n",
    "            # 3) We prepare the dataset, data collator and data loaders\n",
    "            self.split_dataset()\n",
    "            self.set_data_collator()\n",
    "            self.set_loaders()\n",
    "            # 4) We prepare the model, optimizer and scheduler\n",
    "            self.set_model()\n",
    "            if verbose:\n",
    "                print(\n",
    "                  f\"Model has {self.count_parameters()[1]} trainable \"\n",
    "                  f\"parameters out of {self.count_parameters()[0]} total.\"\n",
    "              )\n",
    "            self.set_optimizer_and_scheduler(verbose=verbose)\n",
    "            # 5) We train and validate\n",
    "            self.train_and_validate()\n",
    "\n",
    "        elif mode == \"inference\":\n",
    "            if self.pretrained_model_name_or_path is None:\n",
    "                raise ValueError(\n",
    "                    \"You must provide model_path for inference mode.\"\n",
    "                )\n",
    "            # 1. We load tokenizer + model\n",
    "            self.set_tokenizer()\n",
    "            self.set_model()\n",
    "            # 2. We set the model to evaluation mode\n",
    "            self.model.eval()\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'train' or 'inference'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e93abc1",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273ea86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_demo = LLMFineTuner()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3f6d7",
   "metadata": {},
   "source": [
    "## Download the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab06ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_demo.set_dataset(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9126cf5",
   "metadata": {},
   "source": [
    "## Prepare model inputs\n",
    "\n",
    "The input format to BERT looks like it is  \"over-specified\", especially if you focus on just one type task: sequence classification, word tagging, paraphrase detection, ...  The format:\n",
    "- Add special tokens to the start and end of each sentence.\n",
    "- Pad & truncate all sentences to a single constant length.\n",
    "- Explicitly differentiate real tokens from padding tokens with the \"attention mask\".\n",
    "\n",
    "It looks like that:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1cb5xeqLu_5vPOgs3eRnail2Y00Fl2pCo\" width=\"600\">\n",
    "\n",
    "If you don't want to recreate this kind of inputs with your own hands, you can use the pre-trained tokenizer associated to BERT. Moreover the function `encode_plus` will:\n",
    "- Tokenize the sentence.\n",
    "- Prepend the `[CLS]` token to the start.\n",
    "- Append the `[SEP]` token to the end.\n",
    "- Map tokens to their IDs.\n",
    "- Pad or truncate the sentence to `max_length`\n",
    "- Create attention masks for `[PAD]` tokens.\n",
    "\n",
    "\n",
    "> 💡 *Note:* For computational reasons, we will use the [DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) model, which is a 40% smaller than the original BERT model but still achieve about 95% of the performances of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008c1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_demo.set_tokenizer(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a1193",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"hello my name is kevin\"\n",
    "tok = LLMFineTuner_demo.tokenizer.tokenize(message)\n",
    "print(\"Tokens in the sequence:\", tok)\n",
    "enc = LLMFineTuner_demo.tokenizer.encode(tok)\n",
    "table = np.array(\n",
    "    [\n",
    "        enc,\n",
    "        [LLMFineTuner_demo.tokenizer.ids_to_tokens[w] for w in enc],\n",
    "    ]\n",
    ").T\n",
    "print(\"Encoded inputs:\")\n",
    "print(tabulate(table, headers=[\"Token IDs\", \"Tokens\"], tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567cac2",
   "metadata": {},
   "source": [
    "🚧 **Question** 🚧\n",
    "\n",
    "You noticed special tokens like `[CLS]` and `[SEP]` in the sequence. Note how they were added automatically by HuggingFace.\n",
    "\n",
    "- Why are there such special tokens?\n",
    "\n",
    "**Answer - Edoardo as of 08 2025**\n",
    "\n",
    "CLS --> model assumes we are working on a classification task. It is a token used to represent the sentence for the next tasks and it can be considered, after several transformer layers, as a summary representation of the sentences.\n",
    "\n",
    "SEP --> it is the separator token. In this specific case it is not important because we have only 1 sentence. However, it is important when there are several sentences.\n",
    "\n",
    "The SEP token is very important, for example, in case we have a Q&A tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c242b",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "\n",
    "Usual data-processing for torch.\n",
    "\n",
    "**Explanation Edoardo 08 2025**\n",
    "\n",
    "The function below converts raw review text and sentiment lables into a tokenized sequence and numerical labels. Those can be consumed by an Hugging Face Model.\n",
    "Prepare data set for model training. Training will be done with DistilBert on 2000 examples randomly selected and tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecdcf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_demo.split_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0f1c7",
   "metadata": {},
   "source": [
    "**Explanation Edoardo 08 2025**\n",
    "\n",
    "As in the preprocessing function we did not do the padding, we are now doing it. The above code allows to:\n",
    "\n",
    "1. It takes the input_ids (which are different lengths right now).\n",
    "2. Pads them so they all match the longest sequence in that batch (efficient, avoids over-padding).\n",
    "3. Adds an attention mask automatically (1 for real tokens, 0 for padding).\n",
    "4. Converts everything into PyTorch tensors (torch.LongTensor) so the model can use them.\n",
    "\n",
    "The collator will:\n",
    "\n",
    "- Pad the batch dynamically\n",
    "- Add attention mask labels (0 & 1)\n",
    "- Return tensors in the right format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee8380",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_demo.set_data_collator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0fe1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_demo.set_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0d0b7",
   "metadata": {},
   "source": [
    "**What is done above**\n",
    "\n",
    "| Step                   | Status | Code you already have                                               | Purpose                                                           |\n",
    "| ---------------------- | ------ | ------------------------------------------------------------------- | ----------------------------------------------------------------- |\n",
    "| Dataset loaded         | ✅      | `dataset` already exists                                            | Get raw reviews + sentiments                                      |\n",
    "| Shuffle                | ✅      | `dataset = dataset.shuffle()`                                       | Remove order bias                                                 |\n",
    "| Subsample              | ✅      | `dataset.select(range(n_samples))`                                  | Work on 2,000 examples only                                       |\n",
    "| Tokenization           | ✅      | `dataset.map(preprocessing_fn, fn_kwargs={\"tokenizer\": tokenizer})` | Convert reviews → token IDs, sentiment → labels                   |\n",
    "| Column pruning         | ✅      | `select_columns([\"input_ids\", \"labels\"])`                           | Keep only relevant fields                                         |\n",
    "| Train/validation split | ✅      | `train_test_split(test_size=0.2)`                                   | Split into train/valid sets                                       |\n",
    "| Data collator          | ✅      | `DataCollator(tokenizer)`                                           | Handles **dynamic padding**, adds attention mask, returns tensors |\n",
    "| Dataloaders            | ✅      | `DataLoader(train_set, ... collate_fn=data_collator)`               | Feed data in mini-batches                                         |\n",
    "\n",
    "**What is missing to run a classification using the above**\n",
    "\n",
    "| Step                                     | What to do                                                                                                                                                            |\n",
    "| ---------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Device setup & reproducibility**       | Select GPU if available (`device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")`), set random seeds for reproducibility (`random`, `numpy`, `torch`). |\n",
    "| **Load model**                           | Use `DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)`, then move it to `device`.                                         |\n",
    "| **Optimizer**                            | Define optimizer, usually `AdamW` with small LR (`2e-5` to `5e-5`) and weight decay (e.g., `0.01`).                                                                   |\n",
    "| **Scheduler (optional but recommended)** | Learning rate warmup + decay, via `get_linear_schedule_with_warmup`.                                                                                                  |\n",
    "| **Training loop**                        | Iterate over `train_dataloader`: forward pass, compute loss, backward pass, gradient clipping, optimizer + scheduler step.                                            |\n",
    "| **Validation loop**                      | After each epoch, run `model.eval()` on `valid_dataloader`: compute val loss, accuracy, F1 score, etc.                                                                |\n",
    "| **Metrics**                              | Track training loss, validation loss, and at least **accuracy** (better also F1 if dataset is imbalanced).                                                            |\n",
    "| **Logging / monitoring**                 | Print metrics per epoch, optionally add progress bars (`tqdm`).                                                                                                       |\n",
    "| **Checkpointing**                        | Save best model & tokenizer: `model.save_pretrained(\"./distilbert-best\")`, `tokenizer.save_pretrained(\"./distilbert-best\")`.                                          |\n",
    "| **Sanity checks**                        | Verify `[CLS]` (id=101) at start, `[SEP]` (id=102) present, `attention_mask` correct; inspect one batch to confirm.                                                   |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f637d98",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "- Loads DistilBERT pretrained on masked language modeling.\n",
    "- Adds a classification head (a linear layer on top of the [CLS] embedding).\n",
    "- Sets num_labels=2 → binary classification.\n",
    "- Moves everything to the device you set (cuda if available). Both the model and the data needs to be on the same devide to interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fc7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_demo.set_model(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34ed54",
   "metadata": {},
   "source": [
    "**Interpretation Output**\n",
    "\n",
    "* DistilBERT itself is pretrained with **masked language modeling** (predict missing words) ;\n",
    "* `DistilBertForSequenceClassification` adds a **randomly initialized classification head** on top ;\n",
    "* The encoder starts from pretrained weights, but the classifier must be **fine-tuned on our sentiment dataset**  ;\n",
    "* During fine-tuning, the head learns to map the `[CLS]` embedding → **positive / negative** ;\n",
    "* Next step: **Train**, then **Use**.\n",
    "\n",
    "Workflow:\n",
    "\n",
    "1. **Fine-tune**\n",
    "2. **Use for inference**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82a881d",
   "metadata": {},
   "source": [
    "## Optimizer and Scheduler\n",
    "\n",
    "**Recap Optimizer**\n",
    "\n",
    "It is an algo that updates the model weights during training (based on the latest computed loss).\n",
    "\n",
    "Adam learns different learning rates for different weights. The decay prevents overfitting by keeping weights to grow too large.\n",
    "\n",
    "**Recap Scheduler**\n",
    "\n",
    "Learning rate = how big the optimizer steps are.\n",
    "\n",
    "We use the scheduler as the weights are random at the beginning and we need to gradually change them.\n",
    "\n",
    "From an **intuitive** perspective:\n",
    "\n",
    "- Optimizer = *how do we adjust the weights based on the loss?*\n",
    "- Scheduler = *how big should we make the steps from now to the next training ?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b23e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_demo.set_optimizer_and_scheduler(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef6ced6",
   "metadata": {},
   "source": [
    "## Training & Validation\n",
    "\n",
    "**Explanation of the theory**\n",
    "\n",
    "*Forward pass*\n",
    "\n",
    "Input: a batch of reviews → tokenized into input_ids and attention_mask.\n",
    "\n",
    "The model:\n",
    "\n",
    "- Looks up embeddings for each token.\n",
    "- Passes them through the DistilBERT encoder (stack of Transformer layers).\n",
    "- Uses the [CLS] token’s hidden state as a representation of the whole sentence.\n",
    "- Feeds that into the classification head (a small linear layer).\n",
    "- Output: logits → raw, unnormalized scores for each class (positive/negative).\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "\\text{logits} = W \\cdot h_{\\text{[CLS]}} + b\n",
    "$$\n",
    "\n",
    "*Loss computation*\n",
    "\n",
    "We compare the logits with the true labels (0 and 1). We calculate the loss function cross entropy:\n",
    "\n",
    "$$\n",
    "L = - \\Big( y \\cdot \\log(\\hat{y}) + (1-y) \\cdot \\log(1-\\hat{y}) \\Big)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{\\text{logits}_i}}{\\sum_j e^{\\text{logits}_j}}\n",
    "$$\n",
    "\n",
    "\n",
    "*Backward pass*\n",
    "\n",
    "Pytorch automatically computes the gradients of the loss with respect to every model parameters. The gradient (or slope) will tell us how to nugde each weight to reduce the loss.\n",
    "\n",
    "- If the gradient is +, we should decrease the weight. And viceversa.\n",
    "\n",
    "*Gradient clipping*\n",
    "\n",
    "If gradients produced risk to be too large, we can clip them to limit their size and ensure stable updates. Mathematically:\n",
    "\n",
    "$$\n",
    "g = min(g,clip  value)\n",
    "$$\n",
    "\n",
    "*Optimizer*\n",
    "\n",
    "Then it is the role of the optimizer to apply the gradients to update weights, including weight decay in our case.\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$$\n",
    "w \\leftarrow w - \\eta \\cdot \\nabla_w L\n",
    "$$\n",
    "\n",
    "With n being the learning rate.\n",
    "\n",
    "*Scheduler*\n",
    "\n",
    "Instead of keeping the learning rate constant with start small, increase it and then decrease it again.\n",
    "\n",
    "*Validation*\n",
    "\n",
    "- Turn off gradients (model.eval() + torch.no_grad()).\n",
    "- Run the model on validation data.\n",
    "- Compute validation loss + metrics (accuracy, F1).\n",
    "\n",
    "This checks if the model is learning general patterns, not just memorizing training data.\n",
    "\n",
    "*Checkpoint*\n",
    "\n",
    "Save weights when validation loss improves. To prevent forgetting a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de860ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_demo.train_and_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc75889",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Classic pattern: the model is getting more confident over epochs. Accuracy creeps up, but validation loss rises (overconfidence on wrong cases). This is mild overfitting / miscalibration at epocs 1 and 3.\n",
    "\n",
    "Saving based on the current validation loss, the current best checkpoint is epoch 2. This is the best choice for generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15424c80",
   "metadata": {},
   "source": [
    "## Sanity Check #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ed58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a single batch from the training dataloader\n",
    "batch = next(iter(LLMFineTuner_demo.train_loader))\n",
    "\n",
    "# Print shapes\n",
    "print(\"Input IDs shape:\", batch[\"input_ids\"].shape)\n",
    "print(\"Attention mask shape:\", batch[\"attention_mask\"].shape)\n",
    "print(\"Labels shape:\", batch[\"labels\"].shape)\n",
    "\n",
    "# Print the first example\n",
    "first_input_ids = batch[\"input_ids\"][0]\n",
    "first_mask = batch[\"attention_mask\"][0]\n",
    "first_label = batch[\"labels\"][0]\n",
    "\n",
    "print(\"\\nFirst input_ids:\", first_input_ids.tolist())\n",
    "print(\"First attention_mask:\", first_mask.tolist())\n",
    "print(\"First label:\", first_label.item())\n",
    "\n",
    "# Convert IDs back to tokens to inspect\n",
    "tokens = LLMFineTuner_demo.tokenizer.convert_ids_to_tokens(first_input_ids)\n",
    "print(\"\\nDecoded tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a728b",
   "metadata": {},
   "source": [
    "- Each batch has 4 samples as expected.\n",
    "- Sequences padded/truncated to length 256, as expected\n",
    "- Attention mask correctly aligns with input ids\n",
    "- Special tokens are present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d0eb0",
   "metadata": {},
   "source": [
    "## Sanity Check 2\n",
    "\n",
    "Validate that special tokens are present in a random batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(LLMFineTuner_demo.train_loader))\n",
    "iid = batch[\"input_ids\"][0]\n",
    "assert (\n",
    "    iid[0].item() == LLMFineTuner_demo.tokenizer.cls_token_id\n",
    "), \"Missing [CLS]\"\n",
    "assert (\n",
    "    (iid == LLMFineTuner_demo.tokenizer.sep_token_id).any().item()\n",
    "), \"Missing [SEP]\"\n",
    "print(\"Special tokens OK ✔️\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb2009",
   "metadata": {},
   "source": [
    "## Sanity Check 3\n",
    "\n",
    "Check truncation and padding rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053cd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_pad_stats(dataloader, max_len=256):\n",
    "    n, n_trunc, n_pad = 0, 0, 0\n",
    "    for b in dataloader:\n",
    "        input_ids = b[\"input_ids\"]\n",
    "        attn = b[\"attention_mask\"]\n",
    "        # padded examples have any 0 in mask\n",
    "        n_pad += (attn.sum(dim=1) < input_ids.size(1)).sum().item()\n",
    "        # truncated examples exactly hit max_len AND have no padding\n",
    "        n_trunc += ((attn.sum(dim=1) == max_len)).sum().item()\n",
    "        n += input_ids.size(0)\n",
    "    return {\n",
    "        \"total_examples\": n,\n",
    "        \"padded_frac\": n_pad / n,\n",
    "        \"truncated_frac\": n_trunc / n,\n",
    "        \"exact_len_frac\": (\n",
    "            n_trunc / n\n",
    "        ),  # same as truncated_frac with this logic\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = trunc_pad_stats(LLMFineTuner_demo.train_loader, max_len=256)\n",
    "print(json.dumps(stats, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8d8633",
   "metadata": {},
   "source": [
    "*Conclusion check 3*\n",
    "\n",
    "The truncated fraction is a bit high. This means that we need to raise the max length from 256, otherwise long form context is not retained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ae86d3",
   "metadata": {},
   "source": [
    "## Sanity Check 4\n",
    "\n",
    "Spot check a padded example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_padded_example(dataloader, tokenizer):\n",
    "    for b in dataloader:\n",
    "        for i in range(b[\"input_ids\"].size(0)):\n",
    "            attn = b[\"attention_mask\"][i]\n",
    "            if attn[-1].item() == 0:  # ends with padding\n",
    "                ids = b[\"input_ids\"][i]\n",
    "                toks = tokenizer.convert_ids_to_tokens(ids)\n",
    "                print(\"...tokens tail:\", toks[-30:])\n",
    "                print(\"...mask tail:\", attn[-30:].tolist())\n",
    "                return\n",
    "    print(\"No padded example found in this pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e114f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_padded_example(\n",
    "    LLMFineTuner_demo.train_loader, LLMFineTuner_demo.tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a797c058",
   "metadata": {},
   "source": [
    "*Conclusion sanity check 4*\n",
    "\n",
    "Short reviews are padded up to the batch max length and attention mask correctly ignores padding, hence there is no pollution of the context as the model will not read the pad tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c87a3c5",
   "metadata": {},
   "source": [
    "## Inference helper\n",
    "\n",
    "We can, with it, feed raw text and get sentiment predictions from our fine tuned DistilBert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f57aa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    json.dumps(\n",
    "        LLMFineTuner_demo.predict_sentiment(\n",
    "            \"I absolutely loved this movie, it was fantastic!\"\n",
    "        ),\n",
    "        indent=4,\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    json.dumps(\n",
    "        LLMFineTuner_demo.predict_sentiment(\n",
    "            \"This was the worst film I have ever seen.\"\n",
    "        ),\n",
    "        indent=4,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaf6411",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./distilbert-best\"\n",
    "LLMFineTuner_1 = LLMFineTuner(pretrained_model_name_or_path=model_path)\n",
    "LLMFineTuner_1.set_tokenizer()\n",
    "LLMFineTuner_1.set_model()\n",
    "LLMFineTuner_1.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc503123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    json.dumps(\n",
    "        LLMFineTuner_1.predict_sentiment(\n",
    "            \"I absolutely loved this movie, it was fantastic!\"\n",
    "        ),\n",
    "        indent=4,\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    json.dumps(\n",
    "        LLMFineTuner_1.predict_sentiment(\n",
    "            \"This was the worst film I have ever seen.\"\n",
    "        ),\n",
    "        indent=4,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba510121",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "We now have:\n",
    "\n",
    "1. Preprocessing & batching\n",
    "2. Training and validation\n",
    "3. Save of the best model\n",
    "4. Inference on new text (single ones, not batch in the case above)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac66baa",
   "metadata": {},
   "source": [
    "## Batch Inference Helper - Example of use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a787c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [\n",
    "    \"I absolutely loved this movie, it was fantastic!\",\n",
    "    \"This was the worst film I have ever seen.\",\n",
    "    \"The acting was decent but the story was too slow.\",\n",
    "    \"What a masterpiece - I'd watch it again and again!\",\n",
    "]\n",
    "\n",
    "batch_results = LLMFineTuner_1.predict_batch(reviews)\n",
    "for res in batch_results:\n",
    "    print(json.dumps(res, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224eb226",
   "metadata": {},
   "source": [
    "## Pick New Model\n",
    "\n",
    "We now pass to another model. We move to Roberta, which:\n",
    "\n",
    "- Is larger\n",
    "- Has a different tokenizer\n",
    "- Different pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) We pick the comparison model : roberta-base\n",
    "# other options that could be tested: \"bert-base-uncased\",\n",
    "# \"microsoft/MiniLM-L6-H384-uncased\"\n",
    "model_name = \"roberta-base\"\n",
    "LLMFineTuner_2 = LLMFineTuner(\n",
    "    model_cls=AutoModelForSequenceClassification,\n",
    "    pretrained_model_name_or_path=model_name,\n",
    "    tokenizer_cls=AutoTokenizer,\n",
    ")\n",
    "# 2) We load the dataset\n",
    "LLMFineTuner_2.set_dataset()\n",
    "# 3) We load tokenizer + model (for binary classification)\n",
    "LLMFineTuner_2.set_tokenizer(verbose=True)\n",
    "LLMFineTuner_2.set_model(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params, trainable_params = LLMFineTuner_2.count_parameters()\n",
    "print(\n",
    "    f\"[{model_name}] Total params: {total_params/1e6:.1f}M | \"\n",
    "    f\"Trainable: {trainable_params/1e6:.1f}M | \"\n",
    "    f\"Device: {next(LLMFineTuner_2.model.parameters()).device}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b99cea3",
   "metadata": {},
   "source": [
    "| Property                            | **DistilBERT (base-uncased)**                                                     | **RoBERTa-base**                                       |\n",
    "| ----------------------------------- | --------------------------------------------------------------------------------- | ------------------------------------------------------ |\n",
    "| Layers (Transformer encoder blocks) | **6**                                                                             | **12**                                                 |\n",
    "| Hidden size $d_\\text{model}$        | **768**                                                                           | **768**                                                |\n",
    "| Attention heads                     | **12**                                                                            | **12**                                                 |\n",
    "| Dim per head                        | 64                                                                                | 64                                                     |\n",
    "| FFN/intermediate size               | **3072**                                                                          | **3072**                                               |\n",
    "| Parameters (approx.)                | **\\~66M**                                                                         | **\\~125M**                                             |\n",
    "| Max sequence length                 | 512                                                                               | 514\\* (commonly used as 512)                           |\n",
    "| Positional embeddings               | Learned absolute                                                                  | Learned absolute                                       |\n",
    "| Tokenizer & vocab                   | **WordPiece**, 30,522, *uncased*                                                  | **Byte-level BPE**, 50,265, *cased*                    |\n",
    "| Segment (token type) embeddings     | **Not used** (tokenizer may output, model ignores)                                | **Not used**                                           |\n",
    "| Special tokens                      | `[CLS] [SEP] [PAD] [MASK]`                                                        | `<s> </s> <pad> <mask>`                                |\n",
    "| Pretraining objective               | **Masked LM** + **distillation** from BERT-base (adds KL + cosine losses; no NSP) | **Masked LM only**, **dynamic masking**; **no NSP**    |\n",
    "| Pretraining corpora (high-level)    | Wikipedia + BookCorpus (via BERT teacher)                                         | Larger mix (BookCorpus, CC-News, OpenWebText, Stories) |\n",
    "| Typical inference speed             | **Faster** (half the depth)                                                       | Slower vs DistilBERT (deeper)                          |\n",
    "| Typical memory/VRAM                 | **Lower**                                                                         | Higher                                                 |\n",
    "| Practical trade-off                 | Efficiency with \\~95–97% of BERT-base accuracy                                    | Strong baseline accuracy; heavier & costlier           |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd387b52",
   "metadata": {},
   "source": [
    "**Quick takeaways**:\n",
    "\n",
    "- Capacity: RoBERTa-base has ~2× layers and ~2× params vs DistilBERT → generally higher ceiling on accuracy.\n",
    "\n",
    "- Efficiency: DistilBERT is much lighter/faster; good when latency/VRAM matter.\n",
    "\n",
    "- Tokenization: WordPiece (uncased) vs byte-level BPE (cased) can affect handling of rare/Unicode tokens.\n",
    "\n",
    "- Objectives: DistilBERT inherits knowledge via distillation; RoBERTa relies on stronger MLM with dynamic masking and larger corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7dc914",
   "metadata": {},
   "source": [
    "## Rebuild dataset & loaders for the new model\n",
    "\n",
    "We now need to use the new tokenizer, rebuild the collator, and the data loaders. In this way we will be able to compare the two models properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6629402",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_2.split_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edeec59",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "\n",
    "- Tokenization differences matter: RoBERTa uses byte-level BPE and different specials (<s>, </s>, <pad>, <mask>).\n",
    "\n",
    "- Padding ID differs: RoBERTa’s pad id is typically 1 (DistilBERT/BERT is 0). Using tokenizer.pad avoids hard-coding this and prevents subtle bugs in masks.\n",
    "\n",
    "- Keep MAX_LEN consistent with the previous run (256) to ensure a fair comparison; otherwise you confound capacity with context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df832800",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_2.set_data_collator()\n",
    "LLMFineTuner_2.set_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c7a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Pad token ID: {LLMFineTuner_2.tokenizer.pad_token_id}\\n\"\n",
    "    f\"Special tokens:\\n {json.dumps(LLMFineTuner_2.tokenizer.special_tokens_map, indent=2)}\\n\"\n",
    "    f\"Train, valid sizes: {LLMFineTuner_2.n_train}, {LLMFineTuner_2.n_valid}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95afcf8",
   "metadata": {},
   "source": [
    "**Takeaway**\n",
    "\n",
    "The output looks correct for RoBERTa, because:\n",
    "\n",
    "- Pad id = 1 (expected for RoBERTa).\n",
    "- Specials: cls_token = s, sep_token = eos = /s\n",
    "- Split sizes match your 80/20 (1600 / 400).\n",
    "\n",
    "We now need the **optimizer** and the **scheduler** for the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_2.set_optimizer()\n",
    "LLMFineTuner_2.set_scheduler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210fba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[{model_name}] steps={LLMFineTuner_2.num_total_steps}, warmup={LLMFineTuner_2.num_warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b0f07",
   "metadata": {},
   "source": [
    "**Insights**\n",
    "\n",
    "Keeping LR, weight decay, epochs consistent with DistilBERT previously makes the comparison fair. If RoBERTa underperforms at 2e-5, try 1e-5 (larger models sometimes like smaller LR)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbcbfef",
   "metadata": {},
   "source": [
    "## Theoretical Insights\n",
    "\n",
    "*Forward pass*\n",
    "\n",
    "Input sequence is tokenized into IDs, embedded, and passed through the transformer encoder.\n",
    "\n",
    "The CLS for Bert or s for Berta is used for classification.\n",
    "\n",
    "The classification head (linear layer) produces logits:\n",
    "\n",
    "$$\n",
    "z = W h_{\\text{CLS}} + b\n",
    "$$\n",
    "\n",
    "*Softmax to probabilities*\n",
    "\n",
    "Convert logits into probabilities for each class (binary in our case):\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\frac{e^{z_i}}{\\sum_{j=1}^C e^{z_j}}\n",
    "$$\n",
    "\n",
    "where C = 2 classes (positive and negative)\n",
    "\n",
    "*Loss Function*\n",
    "\n",
    "For a true label y:\n",
    "\n",
    "$$\n",
    "L = - \\sum_{i=1}^C y_i \\cdot \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "In the binary case:\n",
    "\n",
    "$$\n",
    "L = - \\Big( y \\cdot \\log(\\hat{y}_1) + (1-y) \\cdot \\log(\\hat{y}_0) \\Big)\n",
    "$$\n",
    "\n",
    "*Backpropagation*\n",
    "\n",
    "Gradients of the loss w.r.t. each weight are computed by the chain rule:\n",
    "\n",
    "$$\n",
    "\\nabla_w L = \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "*Gradient clipping*\n",
    "\n",
    "To stabilize the training, gradients are clipped as:\n",
    "\n",
    "$$\n",
    "g \\leftarrow \\frac{g}{\\max(1, \\tfrac{\\|g\\|}{\\tau})}\n",
    "$$\n",
    "\n",
    "*Optimizer update*\n",
    "\n",
    "AdamW maintains running averages of the gradients and squared gradients (m_t and v_t)\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "$$\n",
    "\n",
    "Bias corrected estimates are:\n",
    "\n",
    "$$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad\n",
    "\\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "$$\n",
    "\n",
    "Weight updates with decay are:\n",
    "\n",
    "$$\n",
    "w_t = w_{t-1} - \\eta \\cdot \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} - \\eta \\cdot \\lambda w_{t-1}\n",
    "$$\n",
    "\n",
    "*Learning rate scheduling*\n",
    "\n",
    "Learning rate evolves as:\n",
    "\n",
    "$$\n",
    "\\text{lr}(t) =\n",
    "\\begin{cases}\n",
    "  \\eta \\cdot \\frac{t}{T_\\text{warmup}} & \\text{if } t < T_\\text{warmup} \\\\\\\\\n",
    "  \\eta \\cdot \\left(1 - \\frac{t - T_\\text{warmup}}{T_\\text{total} - T_\\text{warmup}}\\right) & \\text{if } t \\geq T_\\text{warmup}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "With two phases, warm up and decay.\n",
    "\n",
    "where n is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65425e3d",
   "metadata": {},
   "source": [
    "## Validate and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLMFineTuner_2.train_and_validate(save_dir=\"./roberta-best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dc75bf",
   "metadata": {},
   "source": [
    "## Evaluation Metrics (Accuracy, F1, Confusion Matrix)\n",
    "\n",
    "For both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ff64b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_distilBert = LLMFineTuner(\n",
    "    pretrained_model_name_or_path=\"./distilbert-best\"\n",
    ")\n",
    "metrics_model_distilBert = model_distilBert.evaluate()\n",
    "print(\"==== DISTILBERT METRICS ====\")\n",
    "print(json.dumps(metrics_model_distilBert, indent=4))\n",
    "\n",
    "model_roberta = LLMFineTuner(pretrained_model_name_or_path=\"./roberta-best\")\n",
    "metrics_model_roberta = model_roberta.evaluate()\n",
    "print(\"==== ROBERTA METRICS ====\")  \n",
    "print(json.dumps(metrics_model_roberta, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
