{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j86DqgZL0DKv"
      },
      "source": [
        "# Training generative models\n",
        "\n",
        "<p align=\"center\">\n",
        "  <a href=\"https://colab.research.google.com/github/auduvignac/llm-finetuning/blob/main/notebooks/corrections/training_tinyllama.ipynb\" target=\"_blank\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Ouvrir dans Google Colab\"/>\n",
        "  </a>\n",
        "</p>\n",
        "\n",
        "In this notebook, we are going to showcase quickly how to train large generative models.\n",
        "\n",
        "The goal of this lab is also to demonstrate that there exists a lot of pre-built methods that can automate common tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPYz3b8k2FBI"
      },
      "source": [
        "## Download requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrz3cr_W2OHH"
      },
      "source": [
        "First install a library necessary to run quantized models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "582BktNnz76l"
      },
      "outputs": [],
      "source": [
        "pip install -U accelerate==1.8.1 peft==0.15.0 bitsandbytes==0.46.0 transformers==4.52.4 datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGI3qVAS2cJ8"
      },
      "source": [
        "## Load a generative model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxYRcDiM3Drc"
      },
      "source": [
        "We are going to load a generative model. Here we make use of heavy optimization technique to make sure everything fits on Collab GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOWcL0LU3JYt"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=\"float16\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaylX_8eUIgW"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj-rohWW3sEN"
      },
      "source": [
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Experiment with the generation of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXKueBQI3rxs"
      },
      "outputs": [],
      "source": [
        "text = \"Paris is the capital of\"\n",
        "DEVICE= \"cuda:0\"\n",
        "\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "outputs = model.generate(**inputs, max_new_tokens=200, temperature=1.0, do_sample=True)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0t2lFH7QkkG"
      },
      "source": [
        "## Training\n",
        "\n",
        "First load an instruction dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmTDX58FRM8s"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def process_data(sample):\n",
        "    return tokenizer(sample[\"text\"])\n",
        "\n",
        "data = load_dataset(\"tatsu-lab/alpaca\", )\n",
        "data = data.map(process_data, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RV7WuvRYZf5"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv0XQgN1Xc55"
      },
      "outputs": [],
      "source": [
        "print(data[\"train\"][0][\"text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtvdDn0XXYka"
      },
      "source": [
        "Since this model is quite big, we are going to reduce the number of trainable parameters.\n",
        "\n",
        "To do that, we use **Parameter Efficient Fine-Tuning** (PEFT).\n",
        "\n",
        "In PEFT, we do not tune the full model. Only a small subsets of the parameters are trained, while the others are frozen (i.e., not updated during training).\n",
        "\n",
        "ðŸš§ **TODO** ðŸš§\n",
        "\n",
        "Explain why PEFT reduces the memory cost.\n",
        "\n",
        "In **full fine-tuning**, all model parameters are updated. This requires storing:\n",
        "\n",
        "* the model weights,\n",
        "* gradients for each parameter,\n",
        "* optimizer states (often doubling memory, e.g., Adam keeps momentum and variance).\n",
        "\n",
        "As a result, the total memory footprint can reach **3â€“4Ã— the size of the model**.\n",
        "\n",
        "In **parameter-efficient fine-tuning**, such as **LoRA** or **adapters**, the **original model weights are frozen** and only small additional modules are trained. This means:\n",
        "\n",
        "* no gradients or optimizer states are stored for the frozen weights,\n",
        "* only a tiny fraction (often <1â€“5%) of parameters require training memory.\n",
        "\n",
        "Consequently, PEFT drastically reduces GPU memory usage, making fine-tuning large language models feasible on much smaller hardware.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXZFxSqSQmCh"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "# This determines automatically which module can be used for quantized training inside the model\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6KYW_ifQuY0"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRmENLiicfBw"
      },
      "source": [
        "The cell below adds a small amount of extra-paramter to the model using Lora technique, that is described here: https://arxiv.org/abs/2106.09685"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXqf9mqAQ44i"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Use default config\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sspsxxn7dRAb"
      },
      "source": [
        "We now use the Trainer class of HuggingFace. On simple installations it can be very effective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcTqhPPATwHK"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=16,\n",
        "        gradient_accumulation_steps=1,\n",
        "        max_steps=100,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK9aujFIXMdu"
      },
      "outputs": [],
      "source": [
        "text = \"### Instruction:\\nPropose an outdoor activity.\\n\\n### Response:\\n\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "model.config.use_cache = True\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=200, temperature=1.0, do_sample=True)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "llm-finetuning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
